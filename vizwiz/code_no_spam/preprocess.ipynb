{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import json\n",
    "\n",
    "data_dir = '../data'\n",
    "dataset_name = 'vizwiz'\n",
    "dataset = []\n",
    "with open(os.path.join(data_dir, 'crowd_results_%s.csv'%dataset_name) , 'r') as csvfile:\n",
    "    fid = csv.reader(csvfile, delimiter=',')\n",
    "    column_names = ('qid', 'ans_dis_labels', 'src_dataset', 'ans_type', 'image', 'question', 'answers')\n",
    "    for i, row in enumerate(fid):\n",
    "        if i == 0: \n",
    "            continue      \n",
    "        data = (row[0], map(int, row[1:11]), row[11], row[12], row[13], \n",
    "                row[14], row[15:25])\n",
    "        data = dict(zip(column_names, data))\n",
    "        dataset.append(data)\n",
    "json.dump(dataset, open(os.path.join(data_dir, dataset_name + '.json'), 'w'), ensure_ascii=False)\n",
    "\n",
    "n_train = int(len(dataset) * 0.65)\n",
    "n_test = int(len(dataset) * 0.25)\n",
    "n_val = len(dataset) - n_train - n_test\n",
    "\n",
    "json.dump(dataset[:n_train], \n",
    "          open(os.path.join(data_dir, dataset_name + '_train.json'), 'w'), ensure_ascii=False)\n",
    "\n",
    "json.dump(dataset[n_train:n_train + n_val], \n",
    "          open(os.path.join(data_dir, dataset_name + '_val.json'), 'w'), ensure_ascii=False)\n",
    "\n",
    "json.dump(dataset[:n_train + n_val], \n",
    "          open(os.path.join(data_dir, dataset_name + '_trainval.json'), 'w'), ensure_ascii=False)\n",
    "\n",
    "json.dump(dataset[n_train + n_val:], \n",
    "          open(os.path.join(data_dir, dataset_name + '_test.json'), 'w'), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30004, 19502, 7501, 3001)\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset), n_train, n_test, n_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('test:', [1804, 2425, 1240, 359, 5643, 392, 4986, 5521, 113, 32])\n",
      "('data:', [7160, 9729, 4852, 1486, 22773, 1575, 20132, 22284, 418, 143])\n",
      "[0.25 0.25 0.26 0.24 0.25 0.25 0.25 0.25 0.27 0.22]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "\n",
    "test_set = dataset[n_train + n_val:]\n",
    "\n",
    "tmp1 = np.array([x['ans_dis_labels'] for x in test_set])\n",
    "tmp1 = tmp1 >= 2\n",
    "\n",
    "print('test:', list(np.sum(tmp1, axis=0)))\n",
    "\n",
    "test_set = dataset\n",
    "tmp2 = np.array([x['ans_dis_labels'] for x in test_set])\n",
    "tmp2 = tmp2 >= 2\n",
    "print('data:', list(np.sum(tmp2, axis=0)))\n",
    "\n",
    "print(np.sum(tmp1, axis=0) * 1.0 / np.sum(tmp2, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build vocabulary for question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30004/30004 [00:03<00:00, 8293.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sample = 30004\n",
      "max len = 74\n",
      "vocab size: 4535\n"
     ]
    }
   ],
   "source": [
    "# create json file for vocabulary\n",
    "import json\n",
    "import os\n",
    "import nltk\n",
    "from nltk.stem.snowball import *\n",
    "from tqdm import *\n",
    "from collections import Counter, OrderedDict\n",
    "import string\n",
    "\n",
    "data_dir = '../data'\n",
    "dataset_name = 'vizwiz'\n",
    "dataset = json.load(open(os.path.join(data_dir, dataset_name + '.json')), encoding='cp1252')\n",
    "\n",
    "## question\n",
    "q_counter = Counter()\n",
    "n_sample = 0\n",
    "maxlen = 0\n",
    "\n",
    "for one_data in tqdm(dataset):\n",
    "    n_sample += 1\n",
    "    question = one_data['question']\n",
    "    question = question.lower()\n",
    "    tokens = nltk.word_tokenize(question)\n",
    "    token_len = len(tokens)\n",
    "    maxlen = max([maxlen,token_len])\n",
    "    q_counter.update(tokens)\n",
    "print('number of sample = ' + str(n_sample))\n",
    "print('max len = ' + str(maxlen))\n",
    "q_word_counts = [x for x in q_counter.items()]\n",
    "q_word_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "json.dump(q_word_counts, open('q_word_counts.json', \"w\"), indent=2)\n",
    "\n",
    "### build vocabulary based on question\n",
    "vocab = [x[0] for x in q_word_counts if x[1] >= 0]\n",
    "unk_word = '<UNK>'\n",
    "vocab = [unk_word] + vocab\n",
    "vocab = OrderedDict(zip(vocab,range(len(vocab))))\n",
    "json.dump(vocab, open('word2vocab_id.json', 'w'), indent=2)\n",
    "print('vocab size: {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding dim is 300\n",
      "<UNK>\n",
      "k-cup\n",
      "tshirt\n",
      "vizwiz\n",
      "dayquil\n",
      "ok.\n",
      "chobani\n",
      "gevalia\n",
      "..\n",
      "c.d\n",
      "coffeemate\n",
      "bizwiz\n",
      "scallywag\n",
      "gatorades\n",
      "rice-a-roni\n",
      "plextalk\n",
      "self-rising\n",
      "rightside\n",
      "ganics\n",
      "ios5\n",
      "eye-drop\n",
      "scentsy\n",
      "hormell\n",
      "koolaid\n",
      "pop-tarts\n",
      "vocab\n",
      "eyedrop\n",
      "nightquil\n",
      "k-cups\n",
      "htis\n",
      "stouffers\n",
      "schwans\n",
      "temputure\n",
      "benedrill\n",
      "tossimo\n",
      "probook\n",
      "this/\n",
      "4.\n",
      "miles..it\n",
      "multi-tool\n",
      "still-is\n",
      "pasta-roni\n",
      "earpods\n",
      "36x32s\n",
      "screen.okay\n",
      "slowcooker\n",
      "68.\n",
      "diffrent\n",
      "lemon-aid\n",
      "brela\n",
      "gaviscon\n",
      "cragganmore\n",
      "crock-pot\n",
      "turbo-tax\n",
      "nyqyl\n",
      "image.i\n",
      "jailbreakable\n",
      "biz-wiz\n",
      "6.1.3\n",
      "tilex\n",
      "79.54\n",
      "b-a-l-m\n",
      "vitamic\n",
      "ammex\n",
      "veniburg\n",
      "pop-tart\n",
      "wheee\n",
      "i-\n",
      "afraid.i\n",
      "an______\n",
      "baby-ganic\n",
      "1845.\n",
      "frischenmeier\n",
      "lawrys\n",
      "sodapop\n",
      "giftcard\n",
      "aspal\n",
      "papadoms\n",
      "twelfths\n",
      "lawrey\n",
      "ibprofen\n",
      "telephone-style\n",
      "ptr-1\n",
      "15.\n",
      "mini-usb\n",
      "counter-top\n",
      "lipozene\n",
      "anti-itch\n",
      "6790\n",
      "teatree\n",
      "drywin\n",
      "frecuency\n",
      "talking..\n",
      "streetview\n",
      "web-workers\n",
      "discribe\n",
      "anythings\n",
      "purple-ish\n",
      "ibuprofin\n",
      "on..\n",
      "mello.com\n",
      "this..\n",
      "espressp\n",
      "future.i\n",
      "seeing-eye\n",
      "uncrinkle\n",
      "tossamo\n",
      "pacco\n",
      "..color\n",
      "convectional\n",
      "brenburgee\n",
      "..is\n",
      "cumulo-nimbus\n",
      "you.you\n",
      "coca-colas\n",
      "glenburgie\n",
      "sippy-cup\n",
      "victorator\n",
      "atch\n",
      "bibsy\n",
      "babyganics\n",
      "grayest\n",
      "frogro\n",
      "peanutbutter\n",
      "recognizers\n",
      "whatpage\n",
      "smores\n",
      "7910\n",
      "euro-spices\n",
      "yo-\n",
      "buyon\n",
      "madhushala\n",
      "wbat\n",
      "beeded\n",
      "neckless\n",
      "whaa\n",
      "thingee\n",
      "robuck\n",
      "brevil\n",
      "paraflu\n",
      "e.q\n",
      "evenant\n",
      "plentronics\n",
      "ear-pods\n",
      "lybo\n",
      "lunchable\n",
      "need/\n",
      "earing\n",
      "cordillius\n",
      "kleenexes\n",
      "dm360\n",
      "ago..\n",
      "10.7.\n",
      "slow-cooker\n",
      "bzzz\n",
      "tosimo\n",
      "deligence\n",
      "braclet\n",
      "k-ring\n",
      "penedol\n",
      "zyw\n",
      "me.0\n",
      "itouch\n",
      "emulser\n",
      "multicolors\n",
      "15/16th\n",
      "130-some\n",
      "freebeze\n",
      "talk-over\n",
      "vizwhiz\n",
      "they're-\n",
      "thermastat\n",
      "miscon\n",
      "linxsys\n",
      "y=2x\n",
      "terriiphone\n",
      "whispy\n",
      "mccormicks\n",
      "snakpak\n",
      "low-high\n",
      "bizwhiz\n",
      "ricearoni\n",
      "kcup\n",
      "acaundian\n",
      "dehumidify\n",
      "checkmarked\n",
      "___can\n",
      "unpunctured\n",
      "kilobars\n",
      "seasalt\n",
      "hornbocker\n",
      "that/\n",
      "g-chat\n",
      "playdoh\n",
      "615ml\n",
      "thi-\n",
      "wizwzig\n",
      "kirtner\n",
      "motexting\n",
      "promea\n",
      "signery\n",
      "mx602a\n",
      "weiners\n",
      "question.could\n",
      "webworker\n",
      "beeing\n",
      "back-pack\n",
      "rightside-down\n",
      "o.k\n",
      "curey\n",
      "someone-else\n",
      "top-middle\n",
      "zwy\n",
      "veggieheavenmtclair.com\n",
      "1-2-v\n",
      "salt-and-vinegar\n",
      "j-a-n-u-s.\n",
      "f'in\n",
      "something.thanks\n",
      "can..\n",
      "12002fx\n",
      "is-this\n",
      "-of\n",
      "progesso\n",
      "5-2x\n",
      "titties\n",
      "gmail.com\n",
      "greyest\n",
      "scallyway\n",
      "black.oh\n",
      "sure-\n",
      "of.\n",
      "coffe\n",
      "gilbread\n",
      "abibseed\n",
      "hi.is\n",
      "sudoma\n",
      "hydrite\n",
      "poptarts\n",
      "crugenmore\n",
      "hairclip\n",
      "7/8th\n",
      "logo/label\n",
      "hamenton\n",
      "humanware\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_glove_embedding_init(idx2word, glove_file):\n",
    "    word2emb = {}\n",
    "    with open(glove_file, 'r') as f:\n",
    "        entries = f.readlines()\n",
    "    emb_dim = len(entries[0].split(' ')) - 1\n",
    "    print('embedding dim is %d' % emb_dim)\n",
    "    weights = np.zeros((len(idx2word), emb_dim), dtype=np.float32)\n",
    "\n",
    "    for entry in entries:\n",
    "        vals = entry.split(' ')\n",
    "        word = vals[0]\n",
    "        vals = list(map(float, vals[1:]))\n",
    "        word2emb[word] = np.array(vals)\n",
    "    for idx, word in idx2word.items():\n",
    "        if word not in word2emb:\n",
    "            print(word)\n",
    "            continue\n",
    "        weights[idx] = word2emb[word]\n",
    "    return weights, word2emb\n",
    "\n",
    "emb_dim = 300\n",
    "glove_file = 'glove.6B.%dd.txt'%emb_dim\n",
    "idx2word = {v:k for k, v in vocab.items()}\n",
    "weights, word2emb = create_glove_embedding_init(idx2word, glove_file)\n",
    "np.save('glove6b_init_%dd.npy' % emb_dim, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Vocabulary for answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30004/30004 [00:00<00:00, 98097.15it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sample = 30004\n",
      "max len = 10\n",
      "vocab size: 6250\n"
     ]
    }
   ],
   "source": [
    "## answer\n",
    "q_counter = Counter()\n",
    "n_sample = 0\n",
    "maxlen = 0\n",
    "\n",
    "for one_data in tqdm(dataset):\n",
    "    n_sample += 1\n",
    "    tokens = [x.lower() for x in one_data['answers']]\n",
    "    token_len = len(tokens)\n",
    "    maxlen = max([maxlen,token_len])\n",
    "    q_counter.update(tokens)\n",
    "print('number of sample = ' + str(n_sample))\n",
    "print('max len = ' + str(maxlen))\n",
    "q_word_counts = [x for x in q_counter.items()]\n",
    "q_word_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "json.dump(q_word_counts, open('ans_counts.json', \"w\"), indent=2)\n",
    "\n",
    "### build vocabulary\n",
    "vocab = [x[0] for x in q_word_counts if x[1] >= 5]\n",
    "vocab = OrderedDict(zip(vocab,range(len(vocab))))\n",
    "json.dump(vocab, open('ans2id.json', 'w'), indent=2)\n",
    "print('vocab size: {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
